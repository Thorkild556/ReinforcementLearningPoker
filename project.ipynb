{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0974ef",
   "metadata": {},
   "source": [
    "# Simple dice poker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aaa9e0",
   "metadata": {},
   "source": [
    "The rules of the simple poker game:\n",
    "For simplicity do we use only 2 players?\n",
    "First to make it simpler we take out all cards from the deck that is not either an ace, 2, 3, 4, 5\n",
    "Next we define the rules for what is a good hand. For the cards the ace is the best and the 2 is the worst card.\n",
    "next a pair is better than just having a single card, \n",
    "3 pairs is then better than 2\n",
    "4 pairs is better than 3\n",
    "Then pairs with higher cards is better than pairs with lower cards.\n",
    "a straight (in original poker) in this case will be then be the worst hand out of simplicity.\n",
    "\n",
    "we then have 2 actions, stop or continue.\n",
    "if we stop, we loose the current stake.\n",
    "if we continue, we double the stake.\n",
    "Then if we get to 5 cards, the player with the best hand wins.\n",
    "\n",
    "for this example we take into account the opponents action of continuing or stopping in that if we make it to the 5th card, the opponent has not stopped yet, and that might mean that they have good cards.\n",
    "\n",
    "I think we could cut down on the states in that if we have 3 4's out of 4 cards, then the two other cards arent really relevant, and so we have a bigger learning potential.\n",
    "but when do we cut down? because  having a 5 and 2 3's out of 4 cards, is also interesting whether there is a 5 or a 3.\n",
    "\n",
    "if we want to make it interesting we could take into account raising or just knocking to continue. This could be done by taking into account whether the opponent just raised. we could then have x2 the amount of states for whether the opponent just raised or not. But this would make a single round last potentially many actions of raising / not raising. and it could go into a loop where both keep on raising forever. we could make rules to defend this, but it would take a lot more time so for simplicity we dont take raising into account, except for it happens automatically, where in each round it doubles. \n",
    "\n",
    "Now for reward. \n",
    "No matter what you\n",
    "\n",
    "### points:\n",
    "you could argue just to make a state for each (best) hand you have. but lets say you have a 9 and a 10, and there is a nine on the board. knowing you have a 10 makes it less probable that the opponent beats you with 2 10's since you already have one of them, so a sepperate state is neccesary to model this.  \n",
    "\n",
    "for this scenario (online) updating doesnt really matter, since for one episode we can never return to the same state since states flow in one direction.\n",
    "however, i chose to do this anyway as a learning experience, making it slightly more difficult to implement, but making the code easier to copy eg. for another project where it does matter.\n",
    "\n",
    "Sometimes, something counter intuitive happens. Say the board has 10, 10, 9 but you have 1, 2. here it might seem like you are deemed to have a negative value, but actually the action value for continuing in this state is actually positive around 2.3. This could be do to the fact that there is some randomness involved, and it might have learned more from earlier examples where the opponent (itself) had not yet learned, but i think it is actually do to the fact that the algorithm has learned to bluff. if there is a 10 10, on the board, then the probability is lower that the other player has a 10, and thus if you bluff, then if the other player does not have a 10, then at worst you are even, but at best, the opponent folds. In this way the algorithm seems to have learned to exploit the system. On the other hand if the board then had a 10, 9 and 8, the value for continuing is around -12, since there is actually a good probability that the opponent might have at least one of those 3, another pair or at least a 3.\n",
    "\n",
    "### What would it have required to model real poker?:\n",
    "\n",
    "In real poker we would have to model the following:\n",
    "- Also model royal cards\n",
    "- Different models /state spaces pr. amount of players playing\n",
    "- more card combinations like straight and flush\n",
    "- 5 cards layed on the board, adding even more states\n",
    "- model the combinations of the players doing fold, check, raise, all in\n",
    "- model the different possible stakes\n",
    "- potentially model the states of remaining chips that other players have\n",
    "\n",
    "Just from noticing that we have more continuous features, this type of reinforcement learning would not have been adequate in practice, given that we have finite time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11542a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from src.game import Game, RLPlayer\n",
    "from src.strategy import Strategy\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59b10e",
   "metadata": {},
   "source": [
    "## load pre-trained model from memory (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0158b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from memory (optional)\n",
    "with open(\"strat/strat.pkl\", \"rb\") as f:\n",
    "    strat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd6111",
   "metadata": {},
   "source": [
    "## Train a new Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48514d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of learnable values:  31460\n",
      "mean number of visits pr. state:  317.86395422759057\n",
      "Epsilon convergence towards:  6.742816344747503e-07\n",
      "0.13594858573168675\n"
     ]
    }
   ],
   "source": [
    "n_params = 2*(sum([math.comb(11,2)*math.comb(9+i, 0+i) for i in range(4)]))\n",
    "print(\"Total number of learnable values: \",n_params) # the amount of distinct values\n",
    "print(\"mean number of visits pr. state: \", 1e7/n_params)\n",
    "print(\"Alpha convergence towards :\", 1/1+0.1*(1e7/n_params))\n",
    "print(\"Epsilon convergence towards: \", 0.25*(0.95**(2.5e7/1e5)))\n",
    "\n",
    "print(0.25*(0.97**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4d956ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model \n",
    "epsilon_start = 0.25\n",
    "strat = Strategy(n = 2, gamma = 1, alpha = 0.5, decay_rate = 0.1, epsilon = epsilon_start)\n",
    "\n",
    "players = [RLPlayer(strat), RLPlayer(strat)] # giving them the same strat\n",
    "\n",
    "for i in range(int(2.5e7)): #25 mil simulations ~ 40 minutes\n",
    "    if i % 1e5 == 0:\n",
    "        epsilon *=0.95\n",
    "        strat.epsilon = epsilon\n",
    "    game = Game(players)\n",
    "    game.simulate_game()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8be45e",
   "metadata": {},
   "source": [
    "## Save model (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23e56edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"strat/strat.pkl\", \"wb\") as f:\n",
    "    pickle.dump(strat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b26fb4",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e431ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of visits:  1406.0\n",
      "action value for action Continue: -3.618537682921602\n"
     ]
    }
   ],
   "source": [
    "hand = [10,8]\n",
    "board = [9, 9, 6]\n",
    "#action: 0 -> continue, 1 -> fold\n",
    "action = 0\n",
    "\n",
    "print(\"number of visits: \", strat.n_action_updates[len(board)][action][strat._get_state_idx(hand, board)])\n",
    "print(f\"action value for action {\"Continue\" if action==0 else \"fold\"}:\",strat.action_values[len(board)][action][strat._get_state_idx(hand, board)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
